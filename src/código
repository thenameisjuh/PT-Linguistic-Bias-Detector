# -*- coding: utf-8 -*-
"""PT-Linguistic-Bias-Detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JGDGHGBRXjrQpKO0C5cDrdQt62A06qoV

# **Configura√ß√£o do Ambiente e Constru√ß√£o do Dicion√°rio de Refer√™ncia**

Esta c√©lula √© o ponto de partida do projeto. Ela automatiza a instala√ß√£o das bibliotecas de ponta para Processamento de Linguagem Natural (spaCy) e as ferramentas de extra√ß√£o de not√≠cias (newspaper3k). Al√©m disso, ela cria fisicamente o ficheiro dicionario_bias.csv dentro do Colab. Este ficheiro serve como o "c√©rebro" do detetor, contendo os pares de termos (PT-BR vs PT-PT) que ser√£o usados para identificar o vi√©s lexical.
"""

# 1. Instala√ß√£o
!pip install spacy newspaper3k fpdf2 lxml_html_clean
!python -m spacy download pt_core_news_sm

import spacy, csv, os, pandas as pd, numpy as np, matplotlib.pyplot as plt
from newspaper import Article
from fpdf import FPDF

# 2. Cria√ß√£o do Dicion√°rio
conteudo_csv = """pt_br,pt_pt
tela,ecr√£
celular,telem√≥vel
esporte,desporto
usu√°rio,utilizador
equipe,equipa
planejamento,planeamento
√¥nibus,autocarro
trem,comboio
geladeira,frigor√≠fico
suco,sumo
banheiro,casa de banho
grama,relva
registro,registo
projeto,projecto
fato,facto
a√ß√£o,ac√ß√£o
contato,contacto
f√≥ton,fot√£o
carona,boleia
cafezinho,bica
legal,fixe"""

with open('dicionario_bias.csv', 'w', encoding='utf-8') as f:
    f.write(conteudo_csv)

print("‚úÖ Ambiente pronto e Dicion√°rio Criado!")

"""# **O Cora√ß√£o do Algoritmo: Fun√ß√µes de An√°lise Morfossint√°tica**

Aqui definimos a l√≥gica inteligente do detetor. O c√≥digo utiliza o modelo pt_core_news_sm para realizar uma lematiza√ß√£o, ou seja, ele n√£o procura apenas a palavra exata, mas sim a raiz da palavra (ex: identifica "usu√°rios" atrav√©s da raiz "usu√°rio"). A fun√ß√£o principal realiza uma verifica√ß√£o em duas camadas: deteta o vi√©s de vocabul√°rio (letras azuis no gr√°fico) e identifica padr√µes gramaticais t√≠picos de IA, como o uso excessivo de ger√∫ndios (barras douradas).
"""

nlp = spacy.load("pt_core_news_sm")

def baixar_noticia(url, prefixo, nome_base):
    try:
        art = Article(url)
        art.download(); art.parse()
        if not os.path.exists('corpus'): os.makedirs('corpus')
        # Nomeia o ficheiro com a origem (ex: PT_Publico.txt)
        caminho = f"corpus/{prefixo}_{nome_base}.txt"
        with open(caminho, 'w', encoding='utf-8') as f:
            f.write(art.text)
        print(f"üì• Guardado: {caminho}")
    except: print(f"‚ùå Erro ao baixar {url}")

def analisar_texto(texto, dicionario):
    doc = nlp(texto)
    c_bias, c_ger = 0, 0
    palavras = [t for t in doc if not t.is_punct and not t.is_space]
    for t in doc:
        if t.text.lower().strip() in dicionario or t.lemma_.lower().strip() in dicionario:
            c_bias += 1
        if "VerbForm=Ger" in t.morph: c_ger += 1
    dens_b = (c_bias / len(palavras)) * 100 if palavras else 0
    dens_g = (c_ger / len(palavras)) * 100 if palavras else 0
    return dens_b, dens_g

"""# **Alimentando o Sistema: Web Scraping e Cria√ß√£o de Amostras**

Esta c√©lula √© respons√°vel por recolher a mat√©ria-prima para a an√°lise. Ela utiliza t√©cnicas de Web Scraping para entrar em sites de not√≠cias reais e extrair apenas o texto relevante, eliminando an√∫ncios e menus. O c√≥digo organiza as not√≠cias com prefixos espec√≠ficos (PT_, BR_ ou IA_), o que permite ao sistema categorizar automaticamente a origem de cada texto. Tamb√©m inclu√≠mos aqui um "Teste Extremo" para garantir que o algoritmo est√° calibrado e a detetar o vi√©s corretamente.
"""

# Exemplos - Substitui pelos links reais
baixar_noticia("https://www.publico.pt/2026/03/01/mundo/noticia/vamos-conversar-donald-trump-concordou-falar-nova-lideranca-iraniana-2166469?ref=ultimas&cx=page__section", "PT", "Publico")
baixar_noticia("https://www1.folha.uol.com.br/mundo/2026/03/presidente-do-ira-reaparece-organiza-sucessao-e-promete-vinganca.shtml", "BR", "Folha de S.Paulo")

# Cria√ß√£o manual do teste extremo para o gr√°fico
with open('corpus/IA_Teste_Extremo.txt', 'w', encoding='utf-8') as f:
    f.write("O usu√°rio est√° usando o celular na tela do trem enquanto planeja o esporte.")

# Cria√ß√£o manual da noticia IA (ChatGPT)
with open('corpus/IA_Noticia_ChatGPT.txt', 'w', encoding='utf-8') as f:
    f.write("A equipe est√° analisando o planejamento do projeto e realizando testes.")

"""# **O Dashboard de Investiga√ß√£o e Gera√ß√£o de Relat√≥rio Profissional**

√â a fase final onde os dados se transformam em provas visuais. Esta c√©lula percorre todas as not√≠cias recolhidas, calcula as densidades de vi√©s por cada 100 palavras e gera um gr√°fico comparativo de alta resolu√ß√£o (dashboard_final.png). O design √© ajustado para ser esteticamente apelativo e alinhado com a tua identidade visual (Ciano e Dourado). Por fim, o sistema compila tudo num relat√≥rio PDF autom√°tico, pronto para ser apresentado como uma pe√ßa de investiga√ß√£o t√©cnica.
"""

def carregar_dicionario(caminho):
    df = pd.read_csv(caminho, encoding='utf-8')
    return {str(row.iloc[0]).strip().lower(): str(row.iloc[1]).strip().lower() for _, row in df.iterrows()}

bias_lexico = carregar_dicionario('dicionario_bias.csv')
nomes, b_vals, g_vals, categorias = [], [], [], []

for f_nome in sorted(os.listdir('corpus')):
    if f_nome.endswith('.txt'):
        with open(f'corpus/{f_nome}', 'r', encoding='utf-8') as f:
            b, g = analisar_texto(f.read(), bias_lexico)
            nome_limpo = f_nome.replace('.txt', '')
            nomes.append(nome_limpo)
            b_vals.append(b)
            g_vals.append(g)
            categorias.append("PORTUGAL" if "PT_" in nome_limpo else "BRASIL" if "BR_" in nome_limpo else "IA")

# --- GR√ÅFICO AESTHETIC ---
plt.style.use('dark_background')
fig, ax = plt.subplots(figsize=(12, 7))
x = np.arange(len(nomes))
largura = 0.35

ax.bar(x - largura/2, b_vals, largura, label='Vi√©s Lexical', color='#00E5FF', alpha=0.9)
ax.bar(x + largura/2, g_vals, largura, label='Vi√©s Sint√°tico (Ger√∫ndio)', color='#D4AF37', alpha=0.9)

# Anota√ß√µes de Origem sobre as barras
for i, cat in enumerate(categorias):
    ax.text(i, max(b_vals[i], g_vals[i]) + 0.5, cat, ha='center', fontsize=9, fontweight='bold', color='white')

ax.set_ylabel('Ocorr√™ncias por 100 palavras', color='#AAAAAA')
ax.set_title('PT-Linguistic-Bias-Detector | @pixel.cultural', fontsize=18, pad=25, color='white')
ax.set_xticks(x)
ax.set_xticklabels(nomes, rotation=35, ha='right', color='#CCCCCC')
ax.legend(frameon=False, loc='upper left')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('dashboard_aesthetic.png', dpi=300)
plt.show()

# --- GERA√á√ÉO DO PDF PROFISSIONAL ---
class PDF(FPDF):
    def header(self):
        self.set_font('Helvetica', 'B', 16); self.cell(0, 10, 'Relat√≥rio de Variedade Lingu√≠stica', 0, 1, 'C'); self.ln(10)

pdf = PDF()
pdf.add_page()
pdf.image('dashboard_aesthetic.png', x=10, w=190)
pdf.ln(10)
pdf.set_font("Helvetica", 'B', 12)
pdf.cell(50, 10, "Origem", 1); pdf.cell(80, 10, "Ficheiro", 1); pdf.cell(30, 10, "L√©xico %", 1); pdf.cell(30, 10, "Sintaxe %", 1); pdf.ln()
pdf.set_font("Helvetica", size=10)

for i in range(len(nomes)):
    pdf.cell(50, 10, categorias[i], 1)
    pdf.cell(80, 10, nomes[i][:25], 1)
    pdf.cell(30, 10, f"{b_vals[i]:.2f}", 1)
    pdf.cell(30, 10, f"{g_vals[i]:.2f}", 1); pdf.ln()

pdf.output("Relatorio_Final_Bias.pdf")
print("üöÄ Dashboard e PDF Gerados!")

"""# **Melhorias Futuras e Notas de Investiga√ß√£o**

1. **Expans√£o do Corpus e Profundidade Lingu√≠stica**
* *Escalabilidade do Dicion√°rio:* O dicion√°rio atual √© uma prova de conceito. Pretende-se a integra√ß√£o com bases de dados lexicais robustas, como o Portal da L√≠ngua Portuguesa ou o Dicion√°rio Aberto, para abranger arca√≠smos e neologismos.

* *An√°lise de Sentimento Associada:* Implementar um m√≥dulo de an√°lise de sentimento para verificar se o vi√©s lingu√≠stico (PT-BR vs PT-PT) est√° correlacionado com tons emocionais espec√≠ficos em not√≠cias digitais.

2. **Refinamento do Motor de NLP**
* *Dete√ß√£o de "Clich√™s" de IA:* Adicionar a dete√ß√£o de express√µes de transi√ß√£o e n-grams excessivamente utilizados por LLMs (ex: "Em suma", "√â importante notar").

* *Modelos de Transformer:* Evoluir do modelo pt_core_news_sm para modelos baseados em BERT (como o BERTimbau ou o PT-BERT) para uma compreens√£o contextual e sem√¢ntica mais profunda das frases.

3. **Auditoria de Modelos de Linguagem (LLMs)**
* *Benchmarking de Modelos:* Utilizar o sistema para auditar qual o modelo de IA (GPT-4, Claude, Gemini) produz o texto mais alinhado com a norma padr√£o do Portugu√™s Europeu.

* *An√°lise Diacr√≥nica:* Adaptar o c√≥digo para analisar a evolu√ß√£o da escrita jornal√≠stica em Portugal ao longo das d√©cadas, utilizando arquivos digitais como o do Di√°rio de Not√≠cias.

üìå **Notas Importantes para o Utilizador**
* *Limita√ß√µes de Scraping:* Alguns portais de not√≠cias utilizam paywalls ou protocolos de seguran√ßa que podem impedir o acesso autom√°tico via newspaper3k. Nesses casos, recomenda-se o upload manual do ficheiro .txt para a pasta corpus.

* *Rigor das M√©tricas:* Todas as m√©tricas de densidade s√£o normalizadas por cada 100 palavras para garantir que a compara√ß√£o entre textos de diferentes extens√µes seja estatisticamente v√°lida.
"""
